# 2024.10科研月报
## 10.12
### 论文阅读
- [x]  OmniVec2 - A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning
#### 整体结构
![overview_omnivec2.png](/overview_omnivec2.png)
#### 问题
- [x] **为何可以使用12种模态数据进行预训练？**
构建多模态多任务模型，各个模态数据有独有与数据适合的tokenizer和FC layers， 两轮CA之后， 在$h_{iq}$处又有不同的对应模态的不同任务头，来辅助训练。举个例子，例如图片模态，通过ViT-large模型tokenizer，之后经过第一层Transformer和图片独有的FC Layers, 经过cross attention和公用的$g(\cdot)$再与原始Transformer输出结合进入自己专有的CA和任务头来做训练，例如可以做分割、检测、识别，以此使共有的第一层Transformer和第二层Transformer($g(\cdots)$)拥有处理多模态数据的能力。
- [x] **什么是参数共享？ 如何进行，对应图中shared weights**
参数共享现在一般分为三种机制：

1. 硬共享是目前应用最为广泛的共享机制，它把多个任务的数据表示嵌入到同一个语义空间中，再为每个任务使用一任务特定层提取任务特定表示。硬共享实现起来非常简单，适合处理有较强相关性的任务，但遇到弱相关任务时常常表现很差。

2. 软共享为每个任务都学习一个网络，但每个任务的网络都可以访问其他任务对应网络中的信息，例如表示、梯度等。软共享机制非常灵活，不需要对任务相关性做任何假设，但是由于为每个任务分配一个网络，常常需要增加很多参数。

3. 分层共享是在网络的低层做较简单的任务，在高层做较困难的任务。分层共享比硬共享要更灵活，同时所需的参数又比软共享少，但是为多个任务设计高效的分层结构依赖专家经验。

这里shared weights方式说的其实有所隐瞒，这篇文章是一家初创公司做的，估计是为了商用刻意隐瞒细节，从原理上来说不太可能是硬共享，这种方式实在是过于激进了，**大概率是软共享，或者其他的稀疏共享方式**。
- [x] **三种训练方式是什么？如何进行？**

1. 自监督单模态掩膜训练

第一阶段参照bert\mae的思想，随机mask掉一部分token，接一个临时的decoer，利用其他的token预测mask的token可能是什么，这个阶段的预训练每一次都对单一模态进行。训练位置应当是第一个CA之前，至于带不带FC不好说，可能是直接用Transformer decoder来做token预测。

2. 自监督多模态掩膜训练

这一阶段去掉特定任务头，图中$h_{iq}$等，再接一个临时的decoder，在这一阶段中，使用两个模态的数据进行预训练，再经过$g(\cdot)$ 和 CA后，再预测两个分支分别被mask掉的token，这个过程中不用在意模态间的均衡，主要训练的应当是两层CA和$g(\cdot)$

3. 全监督训练

拿出两个模态两个任务进行训练，但更新参数时不是拿所有loss（4个）总和就行更新，而是选择两个loss的和进行更新（没有说具体哪两个，是同一模态的两个任务还是不同模态的两个任务？），这里需要注意模态任务间的均衡配置

- [x] **如何推理？**

移除第一个CA模块，使用单流模型进行推理，相当于说一个transformer encoder + specific fc给多模态输入做编码，然后接入一个具有多模态辨析能力的tansformer encoder进一步编码，最后通过一个transformer decoder(ca) + specific fc输出最终
#### 总结
评价： <span style="color: orange;">&#9733;&#9733;&#9733;&#9734;&#9734;</span> 
理由：SOTA值一颗星、想法值一颗星、写作一颗星，没有代码彻底拉倒，关键点很隐晦的没有写出来，参数共享策略仍不清楚，这块是它基于Uni-Perceiver的改进，故如果需要的话可以进一步阅读，但我感觉用不上，拜拜了。
- [ ] Uni-Perceiver

## 10.13
### 论文阅读  
- [ ] Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction	
一篇基于图神经网络的工作，作者首先举了一个鲜明的例子强调context对于理解对话（conversation）中语句（uttrance）的作用
![utrrance_in_conversation.png](/utrrance_in_conversation.png)
在左右两个对话中都出现了"I know me neither"，但是左边是excited，而右边则是sad，原因在于上下文环境。这里格外强调了以往的工作往往只在意临近的上下文，例如只关注中心句的上句或下句，但是对更之前的，或更之后的长范围的关注度不够，也就是说**特征提取的全局性不够**。此外，此前图神经网络的工作，把每一个unified input当作单独的节点，**限制了提取模态间的具体表示的能力**， 同时**往往忽略了conversation中的时序概念，只在uttrance上分立做，导致对单句效果并不好**。

那么问题就非常简单了，它想解决的是两个事：

1. 从更加high-level的层面去分割句子，以往的工作往往最高是在uttrance上的，就是说不去在乎uttrance和uttrance之间的context，而是关注于uttrance内部word和word之间的context（事实也确实如此）

2. 单个模态的特征提取能力是重要的，并不能直接去把fusion的特征直接input进来当作一个节点，还是要单模态先特征提取。

这两点相对而言第一点创新性比较新颖一点，确实之前的工作没有站在一个更高的尺度去看待分析情感分析这件事儿，考虑到transformer恐怖的长期关联能力，感觉对话越长，对于单句的情感分析将会越精确。但是与之相对的是我觉得工作的实时性会受到影响，但是这在情感分析中似乎也不是什么大事儿。第二个创新点相比而言就要略微差一点，顶多算的上是不同流派之间的争论。

除了这个从conversation角度考虑问题非常好之外，其他都是垃圾，不值得一看……

顺便一提结果，CMU-MOSEI上MSA任务做的其实一般……都不如我自己手写的最简单的transformer加fusion token的版本，我觉得理论来讲可能是因为CMU-MOSEI数据集上


#### 总结
评价： <span style="color: orange;">&#9733;&#9733;&#9733;&#9734;&#9734;</span> 
理由： 开源一颗星，第一个想法两颗星

今儿早晨研究Pull Request规范化的事儿去了，还有git版本控制，下午被那个鸟毛奖学金审核拖了一下午，真的服有些哥们儿，明明知道三四作论文、专利都没用，往那一个劲儿的死命填，弄得我还得死命审……服了…… 颓废的一天，

## 10.14-10.15
### 论文阅读
- [x] Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions

印度姐妹搞出来的一坨……没什么有价值的内容……看完给我五味杂陈，白花俩小时，还不如看点别的……

但是让我注意到了一个比较新的概念，quantum-based fusion， 简单来说就是以量子力学的概念搭建了一个多模态融合网络，
[QAM](https://www.sciencedirect.com/science/article/pii/S1566253520302554)
Highlights

`• A quantum-like multimodal network framework for multimodal sentiment analysis in conversations is proposed.`
`• A quantum interference-inspired multimodal decision fusion method is proposed.`
`• A quantum measurement-inspired strong-weak influence model is proposed.`
`• Extensive experiments have shown its effectiveness.`

但是这一派好像做的结果一般……mosei结果没上80 不过近几年好像也没人玩了…… 诡异

### NVIDIA_GPU架构
#### tensorcore
第一代tensorcore从2017年开始在Volta架构中出现，12nm制程，NVLink2.0，代表型号V100
- [x] 什么是混合精度？
指在底层硬件算子层面，使用半精度（FP16）作为输入和输出，使用全精度（FP32）进行中间结果计算从而不损失过多精度。底层硬件层面其实指的就是Tensor Core， 故GPU上有Tensor Core是使用混合精度训练加速的必要条件。

#### NVLink
![nvlink.png](/nvlink.png)
英伟达开发并推出的一种总线及其通信协议。NVLink采用点对点结构，串列传输，用于CPU和GPU之间的链接，也可用于多个GPU之间的相互链接

- NVSwitch：是一种高速互联技术，同时作为一块独立的NVLink芯片，其提供了高达18路NVLink的接口，可以在多个GPU之间实现高速数据传输

- [x] 分布式训练与NVLink发展
大模型的混合并行
- 横截面并行，按层分开，例如一个GPU跑encoder1和encoder2
- 垂直并行，按张量分开，例如一个GPU只运算一部分矩阵，有点像多头注意力机制，这块给我惊呆了
 
这样就产生一个问题，计算图如何跨节点同步数据进行交互

1. 机器内通信
- 共享内存
- PCIe
- NVLink(直连模式)——GPU与GPU之间直连

2. 机器间通信
- TCP/IP网络
- RDMA网络(直连模式) IB网络、Rocky

通信软件：提供集合通信
- MPI 通用接口，可调用Open-MPI, MVAPICH2, Intel MPI, etc.

- NCCL/HCCL GPU/NPU通信优化库，支持集中式通信


## 10.16-10.17
### 论文阅读
- [ ] KnowleNet: Knowledge Fusion Network for Multimodal Sarcasm Detection

  - **摘要**：讽刺是一种常见的交流形式，通常用于表达轻蔑或嘲讽。在这种交流中，说话者传递的信息与其真实意图相反，通常是为了嘲弄或贬低特定对象。由于讽刺在社交媒体上广泛存在且对机器来说难以检测，因此讽刺检测在自然语言处理领域受到了极大的关注。尽管早期的讽刺检测工作仅依赖于文本数据，但社交媒体上丰富的多模态数据也不容忽视。最近的研究集中在多模态讽刺检测上，通常采用注意力机制和图神经网络来识别图像和文本数据中的相关信息。然而，这些方法可能忽视了先验知识和跨模态语义对比的重要性，而这些因素对于人类的讽刺检测至关重要。本文提出了一种名为 KnowleNet 的新模型，该模型利用 ConceptNet 知识库来引入先验知识，并通过样本级和词级的跨模态语义相似性检测来确定图像与文本的相关性。对比学习也被引入，以改善讽刺（正样本）与非讽刺（负样本）样本在空间分布上的区分。该模型在公开可用的基准数据集上实现了最先进的性能。

  - [x] **关注什么方面？动机是什么？**  
  ---
    文章主要关注讽刺这种情感的检测，基于两点核心假设：
    1. 常识，例如对于一般人而言感冒都是很不好的体验，但是有人说“好一场酣畅淋漓的感冒”，那么他大概率不是要夸这个病生的有多好，而是在生病的过程中他有多惨。因此，引入了 ConceptNet 来做常识的建模。
    2. 在反讽的语境下，图片模态和文本模态往往表达了两种不一样的情感，因此判断两模态的相似程度就可以判定是否是在讽刺语境，此处也利用了仿ConceptNet的设计思路，但是多少是有点诡异了。
    3. 用图生文模型得到图片的caption，然后拿来caption和原始文本放入ConceptNet编码，然后展平输出结果。

  - [x] **ConceptNet是什么？**  
  ---
  	![conceptnet.png](/conceptnet.png)
    ConceptNet 是一个语义网络，帮助计算机理解人类使用的词汇。该网络通过一个稀疏的对称矩阵进行表示。每个词由连接在 ConceptNet 中的其他词表示。网络计算矩阵条目的点对点互信息，并用上下文分布进行平滑处理。负值被截断，以生成正的点对点互信息（PPMI）。结果矩阵的维度通过截断SVD减少到300维，术语和上下文对称地组合成一个单一的词嵌入矩阵。之后，通过最小化目标函数 $\Psi(Q)$，更新新的向量 $\hat{q}_i$，使其接近原始值 $q_i$，并接近图中具有边 $E$ 的相邻词。
    
   	 $$
   	 \Psi(Q) = \sum_{i=1}^{n} \left[ \alpha_i \| q_i - \hat{q}_i \|_2^2 + \sum_{i,j \in E} \beta_{ij} \| q_i - q_j \|_2^2 \right]
   	 $$

    其中 $\alpha_{i}$ 和 $\beta_{ij}$ 是 ConceptNet 中的连接权重值。当原始向量值不存在时，$\alpha_i$ 被设置为0。上述过程产生了一个具有510K词汇量的 ConceptNet 词嵌入矩阵。

	- [x] **相似性如何计算？**
  ---
除了上文中的词级语义相似性检测，论文还考虑到了样本级别（换句话说就是对话中的 utterance 级别）。他们认为，不仅文本中的词与图像属性中的词之间的相关性程度很重要，而且整个图像的特征信息与整个文本之间的相关性也对语义一致性的计算至关重要。然而，文本和图像有不同的编码器，导致它们的特征向量不在同一语义空间中。如果简单地计算图像和文本的特征向量，无法获得令人满意的结果。

针对这个问题，我们对文本和图像的特征向量执行矩阵坐标转换。具体而言，我们首先将图像和文本数据的特征向量连接在一起。然后，我们对特征向量进行中心化处理，即减去均值。

$$
T_{tc} = T_b \oplus C,
$$

$$
I = \{m_1, m_2, \ldots, m_n\},\bar{m} = \frac{1}{n} \sum_{i=1}^{n} m_i
$$

$$
T_{tc} = \{t_1, t_2, \ldots, t_n\},\bar{t} = \frac{1}{n} \sum_{i=1}^{n} t_i.
$$


$$
X = (T_{tc} \oplus I)^T = (t_1, t_2, \ldots, t_n, m_1, m_2, \ldots, m_n).
$$

然后，计算其特征值和对应的特征向量的协方差矩阵。

$$
A = \frac{1}{n-1} XX^T = \begin{pmatrix}
\frac{1}{n-1} \sum_{i=1}^{n}(t_i - \bar{t})^2 & \cdots & \frac{1}{n-1} \sum_{i=1}^{n}(t_i - \bar{t})(m_i - \bar{m}) \\
\vdots & \ddots & \vdots \\
\frac{1}{n-1} \sum_{i=1}^{n}(t_i - \bar{t})(m_i - \bar{m}) & \cdots & \frac{1}{n-1} \sum_{i=1}^{n}(m_i - \bar{m})^2
\end{pmatrix}.
$$

可以通过上述方法计算特征矩阵 \(A\)，然后通过特征值分解方法求解特征值和特征向量以获得 $Q$ 和 $\Sigma$。 $Q$ 是由矩阵 $A$ 的特征向量组成的矩阵，$\Sigma$ 是一个对角阵，主对角线上的元素是特征值。我们取 $Q$ 的前 $p$ 列作为变换矩阵 $P \in \mathbb{R}^{(d_r + d_b \times 2) \times p}$(`这里我必须吐槽几句，ConceptNet用的SVD，到这里他用的方法从公式上看是普通的相似对角化，这里还装模作用的取Q的前p列，实际上p在后续中说是300，也就是ConceptNet的参数设置，我真觉得这部分就是完全copy ConceptNet的思路来做的， 而且啰嗦程度令人无语`)
然后

$$
Y_t = T_{tc}W_1P; \quad Y_i = IW_2P. \quad (Y_t, Y_i \in \mathbb{R}^{1 \times p}).
$$

$W$用于将文本和图像的特征向量转换为与 $P$ 相同的维度。 $(W_1 \in \mathbb{R}^{d_b \times 2 \times (d_r + d_b \times 2)}, \quad W_2 \in \mathbb{R}^{d_r \times (d_r + d_b \times 2)})$。

$$
A = Q \Sigma Q^{-1}
$$

$$
D = (Y_t - Y_i)(Y_t - Y_i)^T.
$$

通过特征矩阵 \(P\)，我们可以获得已降维并映射到相同坐标空间的向量 $Y_t$ 和 $Y_i$。然后，我们计算特征向量 $D$（`其实又是一个没有scale的协方差矩阵`） 的空间距离，以表示图像和文本信息的语义相似性。最后，我们使用 Dropout 层来避免过拟合问题，并使用多个全连接层进一步融合每个模态的特征和进行分类。

$$
T_{f1} = D_{drop}(T_b),
$$

$$
T_{f2} = D_{drop}(\sigma(F C(I \oplus C))),
$$

$$
\hat{y} = \sigma(\sigma(F C(T_{f1} \oplus T_{f2}) \oplus \sigma(F C(F_{ta})) \oplus \sigma(F C(D))),
$$

其中 $(D_{drop}$ 表示 Dropout 层，$F C$ 表示全连接层。 $\sigma$ 是 ReLU 激活函数。 $\oplus$ 表示连接操作。

### 总结

评价： <span style="color: orange;">&#9733;&#9733;&#9733;&#9734;&#9734;</span> 
理由： 没开源扣一分，整体想法很好，可解释性很强，但就是写作的人能力不行，典型的金盆子镶屎边，代笔的人实力实在有限，但无奈他身后的人太强了，真给抬出来一篇顶刊，哎，人生的境遇真是变幻莫测。

## 10.18
### 复习线性代数
#### 特征值分解（EVD）

特征值分解（Eigenvalue Decomposition，EVD）是将一个方阵分解为其特征值和特征向量的过程。对于一个 $n \times n$ 的方阵 $A$，EVD 可以表示为：

$$ A = V \Lambda V^{-1} $$

其中：
- $V$ 是由特征向量组成的矩阵。
- $\Lambda$ 是一个对角矩阵，包含了特征值。

##### 计算流程

1. **计算特征值**：
   求解特征方程：
   $\text{det}(A - \lambda I) = 0$
   其中 $\lambda$ 是特征值，$I$ 是单位矩阵。

2. **计算特征向量**：
   对于每个特征值 $\lambda$，求解线性方程：

   $(A - \lambda I)v = 0$

   其中 $v$ 是特征向量。

3. **构造矩阵**：
   将特征向量放入矩阵 $V$，特征值放入对角矩阵 $\Lambda$。

##### 代码举例

使用 NumPy 进行特征值分解的示例代码：

```python
import numpy as np

# 创建一个方阵
A = np.array([[4, -2],
              [1, 1]])

# 计算特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eig(A)

# 构造对角矩阵 Lambda
Lambda = np.diag(eigenvalues)

# 验证 A = V * Lambda * V^(-1)
V = eigenvectors
A_reconstructed = V @ Lambda @ np.linalg.inv(V)

print("原矩阵 A:\n", A)
print("特征值:\n", eigenvalues)
print("特征向量:\n", eigenvectors)
print("重构矩阵 A:\n", A_reconstructed)

```
- [x] 啥时候能/不能EVD？
并不是所有的方阵都可以进行特征值分解（EVD）。

	- 对角化条件：只有当方阵是可对角化的（即存在足够的线性无关的特征向量）时，才能进行EVD。可对角化的矩阵通常具有不同的特征值。

	- 实数特征值：**如果矩阵是实对称的，那么它总是可以进行EVD，并且所有特征值都是实数且具有正交特征向量。**

	- 复杂特征值：对于一般的方阵，如果存在重复的特征值，矩阵可能仍然可以EVD，但特征向量可能不够形成一个完整的基。

	- 不可对角化矩阵：一些矩阵（例如某些伴随矩阵或不满秩的矩阵）不能进行EVD，因为它们没有足够的特征向量。
#### 奇异值分解（SVD）

奇异值分解（Singular Value Decomposition，SVD）是一种将任意矩阵分解为三个矩阵乘积的分解方式。对于任意的矩阵 $A$，SVD 可以表示为：

$$ A = U \Sigma V^T $$

其中：
- $U$ 是一个正交矩阵，包含左奇异向量。
- $\Sigma$ 是一个对角矩阵，包含奇异值。
- $V^T$ 是一个正交矩阵的转置，包含右奇异向量。

##### 计算流程

1. **构造矩阵**：
   给定矩阵 $A$，$A$ 可以是任意形状的矩阵，不要求是方阵。

2. **求解奇异值和奇异向量**：
   通过特定的算法（如 Jacobi 方法或数值迭代），我们可以求出矩阵 $A$ 的奇异值 $\Sigma$ 及其对应的左奇异向量 $U$ 和右奇异向量 $V$。

##### SVD 的性质

- 奇异值是矩阵 $A$ 的特征值的平方根。
- 矩阵 $A$ 的秩等于其奇异值中非零奇异值的个数。
- SVD 可以用于低秩近似，保留最大的几个奇异值来近似矩阵。

##### 代码举例

使用 NumPy 进行奇异值分解的示例代码：

```python
import numpy as np

# 创建一个矩阵
A = np.array([[3, 1, 1],
              [-1, 3, 1]])

# 计算奇异值分解
U, Sigma, VT = np.linalg.svd(A)

# 构造对角矩阵 Sigma
Sigma_matrix = np.zeros((A.shape[0], A.shape[1]))
np.fill_diagonal(Sigma_matrix, Sigma)

# 验证 A = U * Sigma * VT
A_reconstructed = U @ Sigma_matrix @ VT

print("原矩阵 A:\n", A)
print("U 矩阵:\n", U)
print("Sigma 对角矩阵:\n", Sigma_matrix)
print("V^T 矩阵:\n", VT)
print("重构矩阵 A:\n", A_reconstructed)
```
- [x] 所有矩阵都可以SVD吗？

-  [x] 啥是Jacobi 方法？
#### Jacobi 方法详解

Jacobi 方法是一种用于计算对称矩阵特征值和特征向量的迭代方法。它逐步对矩阵进行相似变换，最终将矩阵对角化。
##### Jacobi 方法的步骤

1. **选择最大非对角元素**：
   在当前矩阵中，找到最大绝对值的非对角元素 $A_{pq}$，该元素位于矩阵的第 $p$ 行第 $q$ 列。选择该元素是为了使矩阵尽可能快地逼近对角矩阵。

2. **构造旋转矩阵**：
   构造一个旋转矩阵 $R(p, q, \theta)$，其中旋转角 $\theta$ 用来消去 $A_{pq}$，通过下列公式计算 $\theta$：
   
   $$
   \theta = \frac{1}{2} \arctan \left( \frac{2A_{pq}}{A_{pp} - A_{qq}} \right)
   $$

3. **更新矩阵**：
   使用旋转矩阵 $R$ 对原矩阵 $A$ 进行相似变换：

   $$
   A' = R^T A R
   $$

4. **重复迭代**：
   直到所有非对角元素接近 $0$，得到对角化的矩阵。

##### Jacobi 方法的代码实现

使用 Python 进行 Jacobi 特征值分解的代码实现：

```python
import numpy as np

# 定义旋转矩阵
def jacobi_rotation(A, p, q):
    if A[p, p] == A[q, q]:
        theta = np.pi / 4
    else:
        theta = 0.5 * np.arctan(2 * A[p, q] / (A[p, p] - A[q, q]))

    cos = np.cos(theta)
    sin = np.sin(theta)

    R = np.eye(A.shape[0])
    R[p, p] = cos
    R[q, q] = cos
    R[p, q] = -sin
    R[q, p] = sin

    return R

# Jacobi 特征值分解
def jacobi_evd(A, tol=1e-8, max_iterations=100):
    n = A.shape[0]
    V = np.eye(n)  # 初始化特征向量矩阵

    for _ in range(max_iterations):
        # 寻找绝对值最大的非对角元素
        p, q = np.unravel_index(np.argmax(np.abs(np.triu(A, 1))), A.shape)

        if abs(A[p, q]) < tol:
            break

        # 生成旋转矩阵
        R = jacobi_rotation(A, p, q)

        # 相似变换 A' = R^T A R
        A = R.T @ A @ R
        V = V @ R  # 更新特征向量矩阵

    # A 的对角元素为特征值，V 的列为特征向量
    eigenvalues = np.diag(A)
    eigenvectors = V

    return eigenvalues, eigenvectors

# 示例矩阵
A = np.array([[4, -2, 2],
              [-2, 5, -4],
              [2, -4, 6]])

# 进行 Jacobi 特征值分解
eigenvalues_j, eigenvectors_j = jacobi_evd(A)

print("jacobi特征值:\n", eigenvalues_j)
print("jacobi特征向量:\n", eigenvectors_j)

eigenvalues, eigenvectors = np.linalg.eig(A)

print("特征值:\n", eigenvalues)
print("特征向量:\n", eigenvectors)
```
## 10.19
### 论文阅读
 - [x] EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning
 **我的总结**
  屎, github代码被人质疑完全无法复现出论文中的结果，下降超过10%
 ---
 - wps AI总结：
    ·  研究背景:
    	1. 音频-视觉学习的重要性： 音频和视觉模态在人类感知周围环境时扮演着关键角色，两者之间存在内在的对应关系。通过自监督学习从大规模无标签视频数据中学习这种对应关系已成为深度学习研究社区的主要兴趣之一。
    	2. 音频-视觉对比学习的挑战： 如何在保持两种不同模态对应关系的同时增强表示能力和多样性是音频-视觉对比学习的一个关键挑战。数据增强是丰富表示的有效方式，但在音频-视觉对比学习中应用有限，因为数据增强容易扭曲音频和视觉模态之间的对应关系。
    ·  研究方法:
    	1. 初步： 自监督学习中利用不变性的概念提供自我监督，目标是将增强输入的表示在特征空间中对齐。
    	2. EquiAV框架： 提出一种新颖的框架，通过扩展等变性到音频-视觉学习中，利用基于共享注意力的变换预测器来实现。它允许将不同增强的特征聚合到一个代表性的嵌入中，提供稳健的监督。这一过程在计算上具有最小的开销。
    ·  实验设计:
    	1. 预训练： 模型在AudioSet-2M上进行自监督预训练，不使用标签。音频编码器和视觉编码器使用MAE预训练的ViT-B/16模型进行初始化。
    	2. 数据增强和参数化： 对视觉输入使用典型的数据增强方法。对于音频模态，将音频信号转换为频谱图，然后应用与视觉相同的增强方法。增强信息被编码为实数向量，表示为ta和tv。
    ·  结果分析:
    	1. 主要结果： EquiAV在各种音频-视觉基准测试中超越了现有的自监督预训练方法，包括音频-视觉事件分类和零样本音频-视觉检索任务。
    	2. 消融研究： 通过消融研究验证了框架的有效性。使用变换预测器产生的音频-视觉嵌入的等变性中心点，可以提高零样本检索和微调性能。
    	3. 定性结果： EquiAV能够有效捕捉音频-视觉输入的显著信息。通过随机生成1000个增强向量，选择最接近中心的等变嵌入作为中心点的替代，进一步验证了模型的有效性。
    ·  总体结论:  EquiAV是一个新颖的自监督音频-视觉对比学习框架，它通过引入等变性原理来克服多模态表示学习中应用增强的限制。广泛的定量和定性结果支持了我们方法的有效性。此外，我们的方法可以应用于其他多模态领域，如视觉-语言。我们希望EquiAV能够帮助推动多模态表示学习的界限。

    文献信息：

    ·  标题: 利用等变性进行音频-视觉对比学习
    ·  作者: Jongsuk Kim, Hyeongkeun Lee, Kyeongha Rho, Junmo Kim, Joon Son Chung
    ·  所属机构: 韩国科学技术院(KAIST), 大田, 韩国
    ·  关键词: 自监督学习, 音频-视觉学习, 对比学习, 等变性, 多模态表示学习
    ·  代码链接: https://github.com/JongSuk1/EquiAV
---

- [x] Equivariant Contrastive Learning
	1. 灵感来源
  	这篇文章的灵感来源于对称性在视觉和图像处理中扮演的重要角色。许多数据（如图像）在某些变换下保持不变或变化可预测。传统对比学习方法往往未能充分利用这种对称性，因此作者希望通过引入等变性（Equivariance）来提高学习的有效性。等变性意味着在输入数据经过某种变换后，输出表示会相应变化，保持一定的结构性。

  2. 核心创新点
  	文章的核心创新在于提出了一种新的对比学习框架，利用等变性来改进表示学习。主要创新包括：

  	- 等变对比损失函数：通过设计新的损失函数，使得在应用变换后，正样本的表示与负样本的表示之间的距离得以有效区分。
  	- 几何对称性：将几何对称性引入对比学习，帮助模型理解不同样本之间的关系，提升模型的判别能力。
  	- 多视角学习：通过引入不同视角的对称变换，增强模型对样本变换的鲁棒性。

